{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"mount_file_id":"1t7FYUrzzgr4KQ4Bv3Q_UYfzM4xO5NgwA","authorship_tag":"ABX9TyOk7+btwnjZB+X9urAMkhJh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#YoloV1 Trained"],"metadata":{"id":"8tF43VOTrzOQ"}},{"cell_type":"markdown","source":["##Importing Packages"],"metadata":{"id":"ZawKwz9Dr6Kd"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from collections import Counter\n","import os\n","import pandas as pd\n","from PIL import Image"],"metadata":{"id":"2V-WUSCmr57c","executionInfo":{"status":"ok","timestamp":1723033623134,"user_tz":-180,"elapsed":508,"user":{"displayName":"Maher Abou Dargham","userId":"07562806362716404982"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["##Loss Function"],"metadata":{"id":"KfP9rC2nlBVH"}},{"cell_type":"code","execution_count":22,"metadata":{"id":"E36M6tsNg6OX","executionInfo":{"status":"ok","timestamp":1723033623659,"user_tz":-180,"elapsed":17,"user":{"displayName":"Maher Abou Dargham","userId":"07562806362716404982"}}},"outputs":[],"source":["\"\"\"\n","Implementation of Yolo Loss Function from the original yolo paper\n","\n","\"\"\"\n","\n","\n","class YoloLoss(nn.Module):\n","    \"\"\"\n","    Calculate the loss for yolo (v1) model\n","    \"\"\"\n","\n","    def __init__(self, S=7, B=2, C=20):\n","        super(YoloLoss, self).__init__()\n","        self.mse = nn.MSELoss(reduction=\"sum\")\n","\n","        \"\"\"\n","        S is split size of image (in paper 7),\n","        B is number of boxes (in paper 2),\n","        C is number of classes (in paper and VOC dataset is 20),\n","        \"\"\"\n","        self.S = S\n","        self.B = B\n","        self.C = C\n","\n","        # These are from Yolo paper, signifying how much we should\n","        # pay loss for no object (noobj) and the box coordinates (coord)\n","        self.lambda_noobj = 0.5\n","        self.lambda_coord = 5\n","\n","    def forward(self, predictions, target):\n","        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n","        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n","\n","        # Calculate IoU for the two predicted bounding boxes with target bbox\n","        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n","        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n","        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n","\n","        # Take the box with highest IoU out of the two prediction\n","        # Note that bestbox will be indices of 0, 1 for which bbox was best\n","        iou_maxes, bestbox = torch.max(ious, dim=0)\n","        exists_box = target[..., 20].unsqueeze(3)  # in paper this is Iobj_i\n","\n","        # ======================== #\n","        #   FOR BOX COORDINATES    #\n","        # ======================== #\n","\n","        # Set boxes with no object in them to 0. We only take out one of the two\n","        # predictions, which is the one with highest Iou calculated previously.\n","        box_predictions = exists_box * (\n","            (\n","                bestbox * predictions[..., 26:30]\n","                + (1 - bestbox) * predictions[..., 21:25]\n","            )\n","        )\n","\n","        box_targets = exists_box * target[..., 21:25]\n","\n","        # Take sqrt of width, height of boxes to ensure that\n","        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n","            torch.abs(box_predictions[..., 2:4] + 1e-6)\n","        )\n","        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n","\n","        box_loss = self.mse(\n","            torch.flatten(box_predictions, end_dim=-2),\n","            torch.flatten(box_targets, end_dim=-2),\n","        )\n","\n","        # ==================== #\n","        #   FOR OBJECT LOSS    #\n","        # ==================== #\n","\n","        # pred_box is the confidence score for the bbox with highest IoU\n","        pred_box = (\n","            bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21]\n","        )\n","\n","        object_loss = self.mse(\n","            torch.flatten(exists_box * pred_box),\n","            torch.flatten(exists_box * target[..., 20:21]),\n","        )\n","\n","        # ======================= #\n","        #   FOR NO OBJECT LOSS    #\n","        # ======================= #\n","\n","        #max_no_obj = torch.max(predictions[..., 20:21], predictions[..., 25:26])\n","        #no_object_loss = self.mse(\n","        #    torch.flatten((1 - exists_box) * max_no_obj, start_dim=1),\n","        #    torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n","        #)\n","\n","        no_object_loss = self.mse(\n","            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n","            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n","        )\n","\n","        no_object_loss += self.mse(\n","            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n","            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1)\n","        )\n","\n","        # ================== #\n","        #   FOR CLASS LOSS   #\n","        # ================== #\n","\n","        class_loss = self.mse(\n","            torch.flatten(exists_box * predictions[..., :20], end_dim=-2,),\n","            torch.flatten(exists_box * target[..., :20], end_dim=-2,),\n","        )\n","\n","        loss = (\n","            self.lambda_coord * box_loss  # first two rows in paper\n","            + object_loss  # third row in paper\n","            + self.lambda_noobj * no_object_loss  # forth row\n","            + class_loss  # fifth row\n","        )\n","\n","        return loss"]},{"cell_type":"markdown","source":["##Utils"],"metadata":{"id":"iiRlPJPvlEDF"}},{"cell_type":"code","source":["def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n","    \"\"\"\n","    Calculates intersection over union\n","\n","    Parameters:\n","        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n","        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n","        box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n","\n","    Returns:\n","        tensor: Intersection over union for all examples\n","    \"\"\"\n","\n","    if box_format == \"midpoint\":\n","        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n","        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n","        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n","        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n","        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n","        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n","        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n","        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n","\n","    if box_format == \"corners\":\n","        box1_x1 = boxes_preds[..., 0:1]\n","        box1_y1 = boxes_preds[..., 1:2]\n","        box1_x2 = boxes_preds[..., 2:3]\n","        box1_y2 = boxes_preds[..., 3:4]  # (N, 1)\n","        box2_x1 = boxes_labels[..., 0:1]\n","        box2_y1 = boxes_labels[..., 1:2]\n","        box2_x2 = boxes_labels[..., 2:3]\n","        box2_y2 = boxes_labels[..., 3:4]\n","\n","    x1 = torch.max(box1_x1, box2_x1)\n","    y1 = torch.max(box1_y1, box2_y1)\n","    x2 = torch.min(box1_x2, box2_x2)\n","    y2 = torch.min(box1_y2, box2_y2)\n","\n","    # .clamp(0) is for the case when they do not intersect\n","    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n","\n","    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n","    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n","\n","    return intersection / (box1_area + box2_area - intersection + 1e-6)\n","\n","\n","def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n","    \"\"\"\n","    Does Non Max Suppression given bboxes\n","\n","    Parameters:\n","        bboxes (list): list of lists containing all bboxes with each bboxes\n","        specified as [class_pred, prob_score, x1, y1, x2, y2]\n","        iou_threshold (float): threshold where predicted bboxes is correct\n","        threshold (float): threshold to remove predicted bboxes (independent of IoU)\n","        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n","\n","    Returns:\n","        list: bboxes after performing NMS given a specific IoU threshold\n","    \"\"\"\n","\n","    assert type(bboxes) == list\n","\n","    bboxes = [box for box in bboxes if box[1] > threshold]\n","    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n","    bboxes_after_nms = []\n","\n","    while bboxes:\n","        chosen_box = bboxes.pop(0)\n","\n","        bboxes = [\n","            box\n","            for box in bboxes\n","            if box[0] != chosen_box[0]\n","            or intersection_over_union(\n","                torch.tensor(chosen_box[2:]),\n","                torch.tensor(box[2:]),\n","                box_format=box_format,\n","            )\n","            < iou_threshold\n","        ]\n","\n","        bboxes_after_nms.append(chosen_box)\n","\n","    return bboxes_after_nms\n","\n","\n","def mean_average_precision(\n","    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n","):\n","    \"\"\"\n","    Calculates mean average precision\n","\n","    Parameters:\n","        pred_boxes (list): list of lists containing all bboxes with each bboxes\n","        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n","        true_boxes (list): Similar as pred_boxes except all the correct ones\n","        iou_threshold (float): threshold where predicted bboxes is correct\n","        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n","        num_classes (int): number of classes\n","\n","    Returns:\n","        float: mAP value across all classes given a specific IoU threshold\n","    \"\"\"\n","\n","    # list storing all AP for respective classes\n","    average_precisions = []\n","\n","    # used for numerical stability later on\n","    epsilon = 1e-6\n","\n","    for c in range(num_classes):\n","        detections = []\n","        ground_truths = []\n","\n","        # Go through all predictions and targets,\n","        # and only add the ones that belong to the\n","        # current class c\n","        for detection in pred_boxes:\n","            if detection[1] == c:\n","                detections.append(detection)\n","\n","        for true_box in true_boxes:\n","            if true_box[1] == c:\n","                ground_truths.append(true_box)\n","\n","        # find the amount of bboxes for each training example\n","        # Counter here finds how many ground truth bboxes we get\n","        # for each training example, so let's say img 0 has 3,\n","        # img 1 has 5 then we will obtain a dictionary with:\n","        # amount_bboxes = {0:3, 1:5}\n","        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n","\n","        # We then go through each key, val in this dictionary\n","        # and convert to the following (w.r.t same example):\n","        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n","        for key, val in amount_bboxes.items():\n","            amount_bboxes[key] = torch.zeros(val)\n","\n","        # sort by box probabilities which is index 2\n","        detections.sort(key=lambda x: x[2], reverse=True)\n","        TP = torch.zeros((len(detections)))\n","        FP = torch.zeros((len(detections)))\n","        total_true_bboxes = len(ground_truths)\n","\n","        # If none exists for this class then we can safely skip\n","        if total_true_bboxes == 0:\n","            continue\n","\n","        for detection_idx, detection in enumerate(detections):\n","            # Only take out the ground_truths that have the same\n","            # training idx as detection\n","            ground_truth_img = [\n","                bbox for bbox in ground_truths if bbox[0] == detection[0]\n","            ]\n","\n","            num_gts = len(ground_truth_img)\n","            best_iou = 0\n","\n","            for idx, gt in enumerate(ground_truth_img):\n","                iou = intersection_over_union(\n","                    torch.tensor(detection[3:]),\n","                    torch.tensor(gt[3:]),\n","                    box_format=box_format,\n","                )\n","\n","                if iou > best_iou:\n","                    best_iou = iou\n","                    best_gt_idx = idx\n","\n","            if best_iou > iou_threshold:\n","                # only detect ground truth detection once\n","                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n","                    # true positive and add this bounding box to seen\n","                    TP[detection_idx] = 1\n","                    amount_bboxes[detection[0]][best_gt_idx] = 1\n","                else:\n","                    FP[detection_idx] = 1\n","\n","            # if IOU is lower then the detection is a false positive\n","            else:\n","                FP[detection_idx] = 1\n","\n","        TP_cumsum = torch.cumsum(TP, dim=0)\n","        FP_cumsum = torch.cumsum(FP, dim=0)\n","        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n","        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n","        precisions = torch.cat((torch.tensor([1]), precisions))\n","        recalls = torch.cat((torch.tensor([0]), recalls))\n","        # torch.trapz for numerical integration\n","        average_precisions.append(torch.trapz(precisions, recalls))\n","\n","    return sum(average_precisions) / len(average_precisions)\n","\n","\n","def plot_image(image, boxes):\n","    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n","    im = np.array(image)\n","    height, width, _ = im.shape\n","\n","    # Create figure and axes\n","    fig, ax = plt.subplots(1)\n","    # Display the image\n","    ax.imshow(im)\n","\n","    # box[0] is x midpoint, box[2] is width\n","    # box[1] is y midpoint, box[3] is height\n","\n","    # Create a Rectangle potch\n","    for box in boxes:\n","        box = box[2:]\n","        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n","        upper_left_x = box[0] - box[2] / 2\n","        upper_left_y = box[1] - box[3] / 2\n","        rect = patches.Rectangle(\n","            (upper_left_x * width, upper_left_y * height),\n","            box[2] * width,\n","            box[3] * height,\n","            linewidth=1,\n","            edgecolor=\"r\",\n","            facecolor=\"none\",\n","        )\n","        # Add the patch to the Axes\n","        ax.add_patch(rect)\n","\n","    plt.show()\n","\n","def get_bboxes(\n","    loader,\n","    model,\n","    iou_threshold,\n","    threshold,\n","    pred_format=\"cells\",\n","    box_format=\"midpoint\",\n","    device=\"cpu\",\n","):\n","    all_pred_boxes = []\n","    all_true_boxes = []\n","\n","    # make sure model is in eval before get bboxes\n","    model.eval()\n","    train_idx = 0\n","\n","    for batch_idx, (x, labels) in enumerate(loader):\n","        x = x.to(device)\n","        labels = labels.to(device)\n","\n","        with torch.no_grad():\n","            predictions = model(x)\n","\n","        batch_size = x.shape[0]\n","        true_bboxes = cellboxes_to_boxes(labels)\n","        bboxes = cellboxes_to_boxes(predictions)\n","\n","        for idx in range(batch_size):\n","            nms_boxes = non_max_suppression(\n","                bboxes[idx],\n","                iou_threshold=iou_threshold,\n","                threshold=threshold,\n","                box_format=box_format,\n","            )\n","\n","\n","            #if batch_idx == 0 and idx == 0:\n","            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n","            #    print(nms_boxes)\n","\n","            for nms_box in nms_boxes:\n","                all_pred_boxes.append([train_idx] + nms_box)\n","\n","            for box in true_bboxes[idx]:\n","                # many will get converted to 0 pred\n","                if box[1] > threshold:\n","                    all_true_boxes.append([train_idx] + box)\n","\n","            train_idx += 1\n","\n","    model.train()\n","    return all_pred_boxes, all_true_boxes\n","\n","\n","\n","def convert_cellboxes(predictions, S=7):\n","    \"\"\"\n","    Converts bounding boxes output from Yolo with\n","    an image split size of S into entire image ratios\n","    rather than relative to cell ratios. Tried to do this\n","    vectorized, but this resulted in quite difficult to read\n","    code... Use as a black box? Or implement a more intuitive,\n","    using 2 for loops iterating range(S) and convert them one\n","    by one, resulting in a slower but more readable implementation.\n","    \"\"\"\n","\n","    predictions = predictions.to(\"cpu\")\n","    batch_size = predictions.shape[0]\n","    predictions = predictions.reshape(batch_size, 7, 7, 30)\n","    bboxes1 = predictions[..., 21:25]\n","    bboxes2 = predictions[..., 26:30]\n","    scores = torch.cat(\n","        (predictions[..., 20].unsqueeze(0), predictions[..., 25].unsqueeze(0)), dim=0\n","    )\n","    best_box = scores.argmax(0).unsqueeze(-1)\n","    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n","    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n","    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n","    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n","    w_y = 1 / S * best_boxes[..., 2:4]\n","    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n","    predicted_class = predictions[..., :20].argmax(-1).unsqueeze(-1)\n","    best_confidence = torch.max(predictions[..., 20], predictions[..., 25]).unsqueeze(\n","        -1\n","    )\n","    converted_preds = torch.cat(\n","        (predicted_class, best_confidence, converted_bboxes), dim=-1\n","    )\n","\n","    return converted_preds\n","\n","\n","def cellboxes_to_boxes(out, S=7):\n","    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n","    converted_pred[..., 0] = converted_pred[..., 0].long()\n","    all_bboxes = []\n","\n","    for ex_idx in range(out.shape[0]):\n","        bboxes = []\n","\n","        for bbox_idx in range(S * S):\n","            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n","        all_bboxes.append(bboxes)\n","\n","    return all_bboxes\n","\n","def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    torch.save(state, filename)\n","\n","\n","def load_checkpoint(checkpoint, model, optimizer):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])"],"metadata":{"id":"KQv8DUoyooRA","executionInfo":{"status":"ok","timestamp":1723033623660,"user_tz":-180,"elapsed":17,"user":{"displayName":"Maher Abou Dargham","userId":"07562806362716404982"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["##Model\n"],"metadata":{"id":"MLluT3NIrQVX"}},{"cell_type":"code","source":["\"\"\"\n","Implementation of Yolo (v1) architecture\n","with slight modification with added BatchNorm.\n","\"\"\"\n","\n","\n","\"\"\"\n","Information about architecture config:\n","Tuple is structured by (kernel_size, filters, stride, padding)\n","\"M\" is simply maxpooling with stride 2x2 and kernel 2x2\n","List is structured by tuples and lastly int with number of repeats\n","\"\"\"\n","\n","architecture_config = [\n","    (7, 64, 2, 3),\n","    \"M\",\n","    (3, 192, 1, 1),\n","    \"M\",\n","    (1, 128, 1, 0),\n","    (3, 256, 1, 1),\n","    (1, 256, 1, 0),\n","    (3, 512, 1, 1),\n","    \"M\",\n","    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n","    (1, 512, 1, 0),\n","    (3, 1024, 1, 1),\n","    \"M\",\n","    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n","    (3, 1024, 1, 1),\n","    (3, 1024, 2, 1),\n","    (3, 1024, 1, 1),\n","    (3, 1024, 1, 1),\n","]\n","\n","\n","class CNNBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, **kwargs): # You can specify kernel_size=, stride=x, padding=\n","        super(CNNBlock, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n","        self.batchnorm = nn.BatchNorm2d(out_channels)\n","        self.leakyrelu = nn.LeakyReLU(0.1)\n","\n","    def forward(self, x):\n","        return self.leakyrelu(self.batchnorm(self.conv(x)))\n","\n","\n","class Yolov1(nn.Module):\n","    def __init__(self, in_channels=3, **kwargs):\n","        super(Yolov1, self).__init__()\n","        self.architecture = architecture_config\n","        self.in_channels = in_channels\n","        self.darknet = self._create_conv_layers(self.architecture)\n","        self.fcs = self._create_fcs(**kwargs)\n","\n","    def forward(self, x):\n","        x = self.darknet(x)\n","        return self.fcs(torch.flatten(x, start_dim=1))\n","\n","    def _create_conv_layers(self, architecture):\n","        layers = []\n","        in_channels = self.in_channels\n","\n","        for x in architecture:\n","            if type(x) == tuple:\n","                layers += [\n","                    CNNBlock(\n","                        in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3],\n","                    )\n","                ]\n","                in_channels = x[1]\n","\n","            elif type(x) == str:\n","                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n","\n","            elif type(x) == list:\n","                conv1 = x[0]\n","                conv2 = x[1]\n","                num_repeats = x[2]\n","\n","                for _ in range(num_repeats):\n","                    layers += [\n","                        CNNBlock(\n","                            in_channels,\n","                            conv1[1],\n","                            kernel_size=conv1[0],\n","                            stride=conv1[2],\n","                            padding=conv1[3],\n","                        )\n","                    ]\n","                    layers += [\n","                        CNNBlock(\n","                            conv1[1],\n","                            conv2[1],\n","                            kernel_size=conv2[0],\n","                            stride=conv2[2],\n","                            padding=conv2[3],\n","                        )\n","                    ]\n","                    in_channels = conv2[1]\n","\n","        return nn.Sequential(*layers)\n","\n","    def _create_fcs(self, split_size, num_boxes, num_classes):\n","        S, B, C = split_size, num_boxes, num_classes\n","\n","        # In original paper this should be\n","        # nn.Linear(1024*S*S, 4096),\n","        # nn.LeakyReLU(0.1),\n","        # nn.Linear(4096, S*S*(B*5+C))\n","\n","        return nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(1024 * S * S, 496),\n","            nn.Dropout(0.0),\n","            nn.LeakyReLU(0.1),\n","            nn.Linear(496, S * S * (C + B * 5)),\n","        )"],"metadata":{"id":"GrO4vNPTrRfp","executionInfo":{"status":"ok","timestamp":1723033623660,"user_tz":-180,"elapsed":16,"user":{"displayName":"Maher Abou Dargham","userId":"07562806362716404982"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["##Dataset"],"metadata":{"id":"TFkNTf3zrRzy"}},{"cell_type":"code","source":["\"\"\"\n","Creates a Pytorch dataset to load the Pascal VOC dataset\n","\"\"\"\n","\n","\n","class VOCDataset(torch.utils.data.Dataset):\n","    def __init__(\n","        self, csv_file, img_dir, label_dir, S=7, B=2, C=20, transform=None,\n","    ):\n","        self.annotations = pd.read_csv(csv_file)\n","        self.img_dir = img_dir\n","        self.label_dir = label_dir\n","        self.transform = transform\n","        self.S = S\n","        self.B = B\n","        self.C = C\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, index):\n","        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n","        boxes = []\n","        with open(label_path) as f:\n","            for label in f.readlines():\n","                class_label, x, y, width, height = [\n","                    float(x) if float(x) != int(float(x)) else int(x)\n","                    for x in label.replace(\"\\n\", \"\").split()\n","                ]\n","\n","                boxes.append([class_label, x, y, width, height])\n","\n","        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n","        image = Image.open(img_path)\n","        boxes = torch.tensor(boxes)\n","\n","        if self.transform:\n","            # image = self.transform(image)\n","            image, boxes = self.transform(image, boxes)\n","\n","        # Convert To Cells\n","        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n","        for box in boxes:\n","            class_label, x, y, width, height = box.tolist()\n","            class_label = int(class_label)\n","\n","            # i,j represents the cell row and cell column\n","            i, j = int(self.S * y), int(self.S * x)\n","            x_cell, y_cell = self.S * x - j, self.S * y - i\n","\n","            \"\"\"\n","            Calculating the width and height of cell of bounding box,\n","            relative to the cell is done by the following, with\n","            width as the example:\n","\n","            width_pixels = (width*self.image_width)\n","            cell_pixels = (self.image_width)\n","\n","            Then to find the width relative to the cell is simply:\n","            width_pixels/cell_pixels, simplification leads to the\n","            formulas below.\n","            \"\"\"\n","            width_cell, height_cell = (\n","                width * self.S,\n","                height * self.S,\n","            )\n","\n","            # If no object already found for specific cell i,j\n","            # Note: This means we restrict to ONE object\n","            # per cell!\n","            if label_matrix[i, j, 20] == 0:\n","                # Set that there exists an object\n","                label_matrix[i, j, 20] = 1\n","\n","                # Box coordinates\n","                box_coordinates = torch.tensor(\n","                    [x_cell, y_cell, width_cell, height_cell]\n","                )\n","\n","                label_matrix[i, j, 21:25] = box_coordinates\n","\n","                # Set one hot encoding for class_label\n","                label_matrix[i, j, class_label] = 1\n","        return image, label_matrix"],"metadata":{"id":"yNWqfJuurTcS","executionInfo":{"status":"ok","timestamp":1723033623661,"user_tz":-180,"elapsed":16,"user":{"displayName":"Maher Abou Dargham","userId":"07562806362716404982"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["##Train"],"metadata":{"id":"p_Q_XSt2rp8J"}},{"cell_type":"code","source":["\"\"\"\n","Main file for training Yolo model on Pascal VOC dataset\n","\n","\"\"\"\n","\n","import torch\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","import torchvision.transforms.functional as FT\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","\n","seed = 123\n","torch.manual_seed(seed)\n","\n","# Hyperparameters etc.\n","LEARNING_RATE = 2e-5\n","DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"\n","BATCH_SIZE = 16 # 64 in original paper but I don't have that much vram, grad accum?\n","WEIGHT_DECAY = 0\n","EPOCHS = 1000\n","NUM_WORKERS = 2\n","PIN_MEMORY = True\n","LOAD_MODEL = False\n","LOAD_MODEL_FILE = \"overfit.pth.tar\"\n","IMG_DIR = \"/content/drive/MyDrive/In Mind/Session08_7 31/data/images\"\n","LABEL_DIR = \"/content/drive/MyDrive/In Mind/Session08_7 31/data/labels\"\n","\n","\n","class Compose(object):\n","    def __init__(self, transforms):\n","        self.transforms = transforms\n","\n","    def __call__(self, img, bboxes):\n","        for t in self.transforms:\n","            img, bboxes = t(img), bboxes\n","\n","        return img, bboxes\n","\n","\n","transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor(),])\n","\n","\n","\n","\n","\n","def main():\n","\n","    train_dataset = VOCDataset(\n","        \"/content/drive/MyDrive/In Mind/Session10_8 7/100examples.csv\",\n","        transform=transform,\n","        img_dir=IMG_DIR,\n","        label_dir=LABEL_DIR,\n","    )\n","\n","    test_dataset = VOCDataset(\n","        \"/content/drive/MyDrive/In Mind/Session10_8 7/test.csv\",\n","        transform=transform,\n","        img_dir=IMG_DIR,\n","        label_dir=LABEL_DIR,\n","    )\n","\n","    train_loader = DataLoader(\n","        dataset=train_dataset,\n","        batch_size=BATCH_SIZE,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=PIN_MEMORY,\n","        shuffle=True,\n","        drop_last=True,\n","    )\n","\n","    test_loader = DataLoader(\n","        dataset=test_dataset,\n","        batch_size=BATCH_SIZE,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=PIN_MEMORY,\n","        shuffle=True,\n","        drop_last=True,\n","    )\n","\n","\n","    # Setup model\n","    my_model = Yolov1(split_size=7, num_boxes=2, num_classes=20)\n","\n","    # Setup optimizer\n","    import torch.optim as optim\n","\n","    optimizer = optim.Adam(my_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","\n","    # Setup loss function\n","    loss_function = YoloLoss()\n","\n","    # Create train fn\n","    def training_func(x, y):\n","      y_pred = my_model(x)\n","      loss = loss_function(y_pred, y)\n","\n","      loss.backward()\n","      optimizer.step()\n","      optimizer.zero_grad()\n","      return y_pred\n","\n","    # Create training loop\n","    my_model.train()\n","\n","    for epoch in range(EPOCHS):\n","      for idx2, (x, y) in enumerate(train_loader):\n","        training_func(x, y)\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"gUPUWY6CrqUW","executionInfo":{"status":"error","timestamp":1723033898603,"user_tz":-180,"elapsed":515,"user":{"displayName":"Maher Abou Dargham","userId":"07562806362716404982"}},"outputId":"3cf68fed-20ee-4bd3-fe3a-3aefaff63c75"},"execution_count":27,"outputs":[{"output_type":"error","ename":"EmptyDataError","evalue":"No columns to parse from file","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-56511b97488f>\u001b[0m in \u001b[0;36m<cell line: 109>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-27-56511b97488f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     train_dataset = VOCDataset(\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;34m\"/content/drive/MyDrive/In Mind/Session10_8 7/100examples.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-d746f0e2f30e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csv_file, img_dir, label_dir, S, B, C, transform)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     ):\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1724\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Fail here loudly instead of in cython after reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"]}]}]}